I've always been fascinated by what makes models like GPT tick. So, I decided to go beyond the APIs and build my own from the ground up.

I'm excited to share GPT-Forge! It's my from-scratch implementation of a GPT-style Transformer in PyTorch, trained to generate its own text based on thousands of IMDB movie reviews.

This project was a deep dive into the fundamentals. Instead of just using a pre-built library, I got my hands dirty with the core mechanics to truly understand how they work.

Here are a few highlights:
âœ¨ I built the entire pipeline, from raw text processing and tokenization to a final inference script.
ðŸ§  I implemented the key mechanisms like causal self-attention and positional encoding from scratch.
ðŸ“Š I systematically scaled the model from a 22M to a 52M parameter version and benchmarked the performance improvement.

It was an incredible feeling to see the model start generating coherent text after training it on an NVIDIA A10G. This was a fantastic learning experience in both AI architecture and end-to-end project development.

I've documented the entire journey, including all the code and results, on my GitHub. I'd love for you to check it out!

GitHub Repo: [Link to your GitHub repository]

#AI #MachineLearning #DeepLearning #PyTorch #Transformers #NLP #GPT #Python #Portfolio #DataScience