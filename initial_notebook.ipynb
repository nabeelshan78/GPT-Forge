{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch==2.2.0\n",
    "# !pip install torchtext==0.17.2\n",
    "# !pip install torchdata==0.7.1\n",
    "# !pip install transformers==4.35.2\n",
    "# !pip install seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "from typing import Any, Iterable, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Transformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam, AdamW, Optimizer\n",
    "from torch.optim.lr_scheduler import StepLR, _LRScheduler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 25000, Test size: 25000\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = torch.load(\"data/imdb_dataset.pt\")\n",
    "print(f\"Train size: {len(train_data)}, Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.')\n",
      "==================================================\n",
      "(1, '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.')\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print('=' * 50)\n",
    "print(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
    "special_symbols = ['<unk>', '<pad>', '<|endoftext|>']\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length: 100685\n",
      "Vocabulary saved to: artifacts/vocab.pth\n"
     ]
    }
   ],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, data_sample in data_iter:\n",
    "        yield tokenizer(data_sample)\n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=special_symbols)\n",
    "vocab.set_default_index(UNK_IDX)\n",
    "\n",
    "artifacts_dir = \"artifacts\"\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "vocab_path = os.path.join(artifacts_dir, \"vocab.pth\")\n",
    "torch.save(vocab, vocab_path)\n",
    "\n",
    "print(f\"Vocabulary Length: {len(vocab)}\")\n",
    "print(f\"Vocabulary saved to: {vocab_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 20000, Val: 5000, Test: 25000\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "train_size = int(0.8 * len(train_data))  # 20,000\n",
    "val_size = len(train_data) - train_size  # 5,000\n",
    "train_data, val_data = random_split(train_data, [train_size, val_size])\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_index = lambda text: [vocab(token) for token in tokenizer(text)]\n",
    "index_to_text = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<unk> <pad> <|endoftext|> the . , and a of to '\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_text(torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "END_OF_TEXT_TOKEN = '<|endoftext|>'\n",
    "\n",
    "def get_training_sample(\n",
    "    text: List[Any], \n",
    "    block_size: int\n",
    ") -> Tuple[List[Any], List[Any]]:\n",
    "    \"\"\"\n",
    "    Creates a single (input, target) training sample for a language model.\n",
    "\n",
    "    From a given text, this function extracts a random contiguous block of\n",
    "    `block_size` tokens for the source (X) and the subsequent, shifted\n",
    "    block of tokens for the target (Y).\n",
    "\n",
    "    Args:\n",
    "        text (List[Any]): The input text, represented as a list of tokens.\n",
    "        block_size (int): The desired length of the input/target sequences.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the source sequence and the target sequence.\n",
    "    \"\"\"\n",
    "    text_len = len(text)\n",
    "\n",
    "    # Case 1: The text is long enough to extract a full random block.\n",
    "    # We need at least `block_size + 1` tokens to create a source and a target.\n",
    "    if text_len > block_size:\n",
    "        max_start_idx = text_len - block_size - 1\n",
    "        start_idx = torch.randint(low=0, high=max_start_idx + 1, size=(1,)).item()\n",
    "        end_idx = start_idx + block_size\n",
    "        \n",
    "        src_sequence = text[start_idx : end_idx]\n",
    "        tgt_sequence = text[start_idx + 1 : end_idx + 1]\n",
    "\n",
    "    # Case 2: The text is too short. Use the entire available text.\n",
    "    else:\n",
    "        # The source is the whole text.\n",
    "        src_sequence = text\n",
    "        # The target is the source shifted by one, with an end-of-text token \n",
    "        # appended to create a valid target for the last source token.\n",
    "        tgt_sequence = text[1:] + [END_OF_TEXT_TOKEN]\n",
    "        \n",
    "    return src_sequence, tgt_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_of_tokens=[]\n",
    "\n",
    "for i in range(2):\n",
    "    _ , text = train_data[i]\n",
    "    batch_of_tokens.append(tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  ['this', 'film', 'while', 'at', 'birmingham', 'southern', 'college', 'in', '1975', ',']\n",
      "tgt:  ['film', 'while', 'at', 'birmingham', 'southern', 'college', 'in', '1975', ',', 'when']\n",
      "====================================================================================================\n",
      "src:  ['as', 'much', 'as', 'the', 'central', 'character', 'in', 'this', 'film', '.']\n",
      "tgt:  ['much', 'as', 'the', 'central', 'character', 'in', 'this', 'film', '.', 'in']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    text = batch_of_tokens[i][0:50]\n",
    "    src_sequences, tgt_sequence = get_training_sample(text, 10)\n",
    "    print(\"src: \", src_sequences)\n",
    "    print(\"tgt: \", tgt_sequence)\n",
    "    print('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  ['i', 'saw', 'this', 'film', 'while', 'at', 'birmingham', 'southern', 'college', 'in', '1975', ',', 'when', 'it', 'was', 'shown', 'in', 'combination', 'with', 'the', 'red', 'balloon', '.', 'both', 'films', 'are', 'similar', 'in', 'their', 'dream-like', 'quality', '.', 'the', 'bulk', 'of', 'the', 'film', 'entails', 'a', 'fish', 'swimming', 'happily', 'in', 'his', 'bowl', 'while', 'his', 'new', 'owner', ',']\n",
      "tgt:  ['saw', 'this', 'film', 'while', 'at', 'birmingham', 'southern', 'college', 'in', '1975', ',', 'when', 'it', 'was', 'shown', 'in', 'combination', 'with', 'the', 'red', 'balloon', '.', 'both', 'films', 'are', 'similar', 'in', 'their', 'dream-like', 'quality', '.', 'the', 'bulk', 'of', 'the', 'film', 'entails', 'a', 'fish', 'swimming', 'happily', 'in', 'his', 'bowl', 'while', 'his', 'new', 'owner', ',', '<|endoftext|>']\n",
      "====================================================================================================\n",
      "src:  ['hi', 'all', 'i', 'am', 'a', 'chess', 'enthusiast', 'since', 'the', 'age', 'of', 'about', '6', '.', 'i', 'supposed', 'i', 'am', 'quite', 'obsessed', 'by', 'chess', ',', 'but', 'hopefully', 'not', 'as', 'much', 'as', 'the', 'central', 'character', 'in', 'this', 'film', '.', 'in', 'this', 'film', ',', 'the', 'central', 'character', 'reflects', 'a', 'real', 'chess', 'player', 'called', 'curt']\n",
      "tgt:  ['all', 'i', 'am', 'a', 'chess', 'enthusiast', 'since', 'the', 'age', 'of', 'about', '6', '.', 'i', 'supposed', 'i', 'am', 'quite', 'obsessed', 'by', 'chess', ',', 'but', 'hopefully', 'not', 'as', 'much', 'as', 'the', 'central', 'character', 'in', 'this', 'film', '.', 'in', 'this', 'film', ',', 'the', 'central', 'character', 'reflects', 'a', 'real', 'chess', 'player', 'called', 'curt', '<|endoftext|>']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    text = batch_of_tokens[i][0:50]\n",
    "    src_sequences, tgt_sequence = get_training_sample(text, 50)\n",
    "    print(\"src: \", src_sequences)\n",
    "    print(\"tgt: \", tgt_sequence)\n",
    "    print('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "Source Sequence (Text): ['complexity', '.', 'it', 'is', 'hard', 'to', 'imagine', 'how', 'the', 'director', 'could', \"'\", 've', 'pulled', 'the', 'technical', 'feat', 'back', 'in', '1959']\n",
      "Source Sequence (Indices): [4600, 4, 12, 11, 274, 9, 823, 96, 3, 174, 105, 10, 147, 1860, 3, 1709, 5830, 154, 13, 6396]\n",
      "Source Sequence (Shape): torch.Size([20])\n",
      "Target Sequence (Text): ['.', 'it', 'is', 'hard', 'to', 'imagine', 'how', 'the', 'director', 'could', \"'\", 've', 'pulled', 'the', 'technical', 'feat', 'back', 'in', '1959', '--']\n",
      "Target Sequence (Indices): [4, 12, 11, 274, 9, 823, 96, 3, 174, 105, 10, 147, 1860, 3, 1709, 5830, 154, 13, 6396, 377]\n",
      "Target Sequence (Shape): torch.Size([20])\n",
      "====================================================================================================\n",
      "Sample 1:\n",
      "Source Sequence (Text): ['suicide', 'in', '1924', '.', 'he', 'is', 'famous', 'for', 'a', 'game', 'he', 'played', 'against', 'steinitz', ',', 'where', 'a', 'beautiful', 'combination', 'was']\n",
      "Source Sequence (Indices): [1746, 13, 16738, 4, 31, 11, 789, 20, 7, 502, 31, 260, 434, 49864, 5, 124, 7, 314, 2159, 18]\n",
      "Source Sequence (Shape): torch.Size([20])\n",
      "Target Sequence (Text): ['in', '1924', '.', 'he', 'is', 'famous', 'for', 'a', 'game', 'he', 'played', 'against', 'steinitz', ',', 'where', 'a', 'beautiful', 'combination', 'was', 'played']\n",
      "Target Sequence (Indices): [13, 16738, 4, 31, 11, 789, 20, 7, 502, 31, 260, 434, 49864, 5, 124, 7, 314, 2159, 18, 260]\n",
      "Target Sequence (Shape): torch.Size([20])\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store source and target sequences\n",
    "src_batch, tgt_batch = [], []\n",
    "BATCH_SIZE = 2\n",
    "block_size = 20\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "    # Retrieve the next data point from the training iterator\n",
    "    _, text = train_data[i]\n",
    "\n",
    "    # Generate source and target sequences using the get_sample function\n",
    "    src_sequence_text, tgt_sequence_text = get_training_sample(tokenizer(text), block_size)\n",
    "\n",
    "    # Convert source and target sequences to tokenized vocabulary indices\n",
    "    src_sequence_indices = vocab(src_sequence_text)\n",
    "    tgt_sequence_indices = vocab(tgt_sequence_text)\n",
    "\n",
    "    # Convert the sequences to PyTorch tensors with dtype int64\n",
    "    src_sequence = torch.tensor(src_sequence_indices, dtype=torch.int64)\n",
    "    tgt_sequence = torch.tensor(tgt_sequence_indices, dtype=torch.int64)\n",
    "\n",
    "    # Append the source and target sequences to their respective batches\n",
    "    src_batch.append(src_sequence)\n",
    "    tgt_batch.append(tgt_sequence)\n",
    "\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(\"Source Sequence (Text):\", src_sequence_text)\n",
    "    print(\"Source Sequence (Indices):\", src_sequence_indices)\n",
    "    print(\"Source Sequence (Shape):\", src_sequence.shape)\n",
    "    print(\"Target Sequence (Text):\", tgt_sequence_text)\n",
    "    print(\"Target Sequence (Indices):\", tgt_sequence_indices)\n",
    "    print(\"Target Sequence (Shape):\", tgt_sequence.shape)\n",
    "    print('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 30\n",
    "\n",
    "def collate_batch(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for _, text in batch:\n",
    "        src_sequence, tgt_sequence = get_training_sample(tokenizer(text), BLOCK_SIZE)\n",
    "        src_sequence, tgt_sequence = vocab(src_sequence), vocab(tgt_sequence)\n",
    "      \n",
    "        src_sequence= torch.tensor(src_sequence, dtype=torch.int64)\n",
    "        tgt_sequence = torch.tensor(tgt_sequence, dtype=torch.int64)\n",
    "        \n",
    "        src_batch.append(src_sequence)\n",
    "        tgt_batch.append(tgt_sequence)\n",
    "\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "\n",
    "    return src_batch.to(DEVICE), tgt_batch.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_data, batch_size=2, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 0\n",
      "Source (src) shape: torch.Size([30, 2])\n",
      "Target (tgt) shape: torch.Size([30, 2])\n",
      "====================================================================================================\n",
      "\n",
      "Samples:\n",
      "Sample 1:\n",
      "  source: photographed with that wonderful opening of guinness and his son driving down the champs elysee with the arc de triomphe in the background . unfortunately it goes downhill from there\n",
      "  target: with that wonderful opening of guinness and his son driving down the champs elysee with the arc de triomphe in the background . unfortunately it goes downhill from there .\n",
      "====================================================================================================\n",
      "Sample 2:\n",
      "  source: . and i don ' t believe there are many cynics who would say that people aren ' t capable of change and redemption . this film version portrays all\n",
      "  target: and i don ' t believe there are many cynics who would say that people aren ' t capable of change and redemption . this film version portrays all of\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(dataloader):\n",
    "    print(f\"\\nBatch {idx}\")\n",
    "    src, tgt = batch  # unpack the tuple\n",
    "\n",
    "    print(\"Source (src) shape:\", src.shape)\n",
    "    print(\"Target (tgt) shape:\", tgt.shape)\n",
    "    print('=' * 100)\n",
    "    \n",
    "    print(\"\\nSamples:\")\n",
    "    for i in range(min(2, src.shape[1])):  # only show up to 2 samples\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(\"  source:\", index_to_text(src[:, i]))  \n",
    "        print(\"  target:\", index_to_text(tgt[:, i])) \n",
    "        print('=' * 100)\n",
    "    break  # only inspect first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "In transformers, masking is crucial for ensuring certain positions are not attended to. The function ```generate_square_subsequent_mask``` produces an upper triangular matrix, which ensures that during decoding, a token can't attend to future tokens of target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Raw Attention Scores\n",
      "tensor([[0.8000, 0.2000, 0.9000, 1.4000],\n",
      "        [0.5000, 0.7000, 1.1000, 0.1000],\n",
      "        [1.2000, 0.3000, 0.6000, 0.4000],\n",
      "        [0.9000, 1.5000, 0.8000, 0.2000]])\n",
      "------------------------------\n",
      "Step 2: Causal Mask\n",
      "tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0.]])\n",
      "------------------------------\n",
      "Step 3: Masked Scores (Scores + Mask)\n",
      "tensor([[0.8000,   -inf,   -inf,   -inf],\n",
      "        [0.5000, 0.7000,   -inf,   -inf],\n",
      "        [1.2000, 0.3000, 0.6000,   -inf],\n",
      "        [0.9000, 1.5000, 0.8000, 0.2000]])\n",
      "------------------------------\n",
      "Step 4: Final Attention Weights after Softmax\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4502, 0.5498, 0.0000, 0.0000],\n",
      "        [0.5114, 0.2079, 0.2807, 0.0000],\n",
      "        [0.2368, 0.4314, 0.2142, 0.1176]])\n",
      "------------------------------\n",
      "Sum of each row in the final weights:\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# 1. the raw attention scores (from Q @ K.T)  # [4, 4]\n",
    "raw_scores = torch.tensor([\n",
    "    [0.8, 0.2, 0.9, 1.4],\n",
    "    [0.5, 0.7, 1.1, 0.1],\n",
    "    [1.2, 0.3, 0.6, 0.4],\n",
    "    [0.9, 1.5, 0.8, 0.2]\n",
    "])\n",
    "\n",
    "# the causal mask\n",
    "# This mask prevents positions from attending to subsequent positions.\n",
    "# 0.0 means \"allowed\", -inf means \"prevented\".\n",
    "mask = torch.tensor([\n",
    "    [0.0, float('-inf'), float('-inf'), float('-inf')],\n",
    "    [0.0, 0.0,        float('-inf'), float('-inf')],\n",
    "    [0.0, 0.0,        0.0,        float('-inf')],\n",
    "    [0.0, 0.0,        0.0,        0.0]\n",
    "])\n",
    "\n",
    "print(\"Step 1: Raw Attention Scores\")\n",
    "print(raw_scores)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Step 2: Causal Mask\")\n",
    "print(mask)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# Add the mask to the raw scores\n",
    "masked_scores = raw_scores + mask\n",
    "\n",
    "print(\"Step 3: Masked Scores (Scores + Mask)\")\n",
    "print(masked_scores)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Apply the softmax function to get the final attention weights\n",
    "# dim=1 ensures softmax is applied row-wise.\n",
    "attention_weights = F.softmax(masked_scores, dim=1)\n",
    "\n",
    "print(\"Step 4: Final Attention Weights after Softmax\")\n",
    "print(attention_weights)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Sum of each row in the final weights:\")\n",
    "print(attention_weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz, device=DEVICE):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    src_mask (Causal Mask): This is the \"No Cheating\" rule.\n",
    "    src_padding_mask: This is the \"Ignore Blank Pages\" rule. ignore padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src, device=DEVICE):\n",
    "    src_seq_len = src.shape[0]       # (sequence_length, batch_size)\n",
    "    # This is our \"No Cheating\" mask, filled with 0.0s and -inf to prevent the model from seeing future tokens during self-attention.\n",
    "    src_mask = generate_square_subsequent_mask(src_seq_len)  # (src_seq_len, src_seq_len)\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, src_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "src[0:3, :] = PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask, src_padding_mask = create_mask(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 30]), torch.Size([2, 30]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask.shape, src_padding_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "\n",
    "    emb_size = 512\n",
    "    maxlen = 5000\n",
    "    batch_size = 64\n",
    "    seq_len = 100   (the length of an input sequence in the forward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size, dropout, maxlen=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # pos: (maxlen, 1) -> (5000, 1)\n",
    "        # Creates a column vector of positions [0, 1, ..., 4999]\n",
    "        pos = torch.arange(maxlen).unsqueeze(1)\n",
    "\n",
    "        # i: (emb_size / 2,) -> (256,)\n",
    "        # Creates a row vector for the even dimension indices [0, 2, ..., 510]\n",
    "        i = torch.arange(0, emb_size, 2)\n",
    "\n",
    "        # angle_rates: (maxlen, emb_size / 2) -> (5000, 256)\n",
    "        # Calculates the arguments for sin/cos using broadcasting.\n",
    "        # (5000, 1) / (256,) results in a (5000, 256) matrix.\n",
    "        angle_rates = pos / (10000 ** (i.float() / emb_size))\n",
    "\n",
    "        # pos_encoding: (maxlen, emb_size) -> (5000, 512)\n",
    "        pos_encoding = torch.zeros(maxlen, emb_size)\n",
    "        \n",
    "        # Fills even indices (0, 2, ...) with sin values.\n",
    "        # The slice pos_encoding[:, 0::2] has shape (5000, 256).\n",
    "        pos_encoding[:, 0::2] = torch.sin(angle_rates)\n",
    "\n",
    "        # Fills odd indices (1, 3, ...) with cos values.\n",
    "        # The slice pos_encoding[:, 1::2] has shape (5000, 256).\n",
    "        pos_encoding[:, 1::2] = torch.cos(angle_rates)\n",
    "\n",
    "        # --- Finalizing and Storing ---\n",
    "        # pos_encoding: (maxlen, 1, emb_size) -> (5000, 1, 512)\n",
    "        # Adds a dimension for batch broadcasting in the forward pass.\n",
    "        pos_encoding = pos_encoding.unsqueeze(1)\n",
    "        \n",
    "        # Registers 'pos_encoding' as a buffer. It's part of the model's state\n",
    "        # but not a parameter to be trained.\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        # token_embedding (input): (seq_len, batch_size, emb_size) -> (100, 64, 512)\n",
    "        seq_len = token_embedding.size(0)\n",
    "\n",
    "        # Add positional encoding to token embedding.\n",
    "        # self.pos_encoding[:seq_len, :] slices the buffer to get shape (100, 1, 512).\n",
    "        # Broadcasting adds this to token_embedding (100, 64, 512).\n",
    "        # The result has shape (100, 64, 512).\n",
    "        output = token_embedding + self.pos_encoding[:seq_len, :]\n",
    "        return self.dropout(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token embedding\n",
    "\n",
    "The `TokenEmbedding` class below converts numerical tokens into embeddings:  \n",
    "\n",
    "    * math.sqrt(self.emb_size)\n",
    "    From the original \"Attention Is All You Need\" paper. The output of the embedding lookup is scaled by the square root of the embedding size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        # Creates a lookup table of shape (vocab_size, emb_size)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        # Input 'tokens' shape: (seq_len, batch_size)\n",
    "        # Output shape: (seq_len, batch_size, emb_size)\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom GPT Model Architecture\n",
    "\n",
    "The `CustomGPTModel` class defines a transformer-based model architecture for generative pre-trained models. This model aims to generate text and perform various NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom GPT-style Transformer model for language generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 embed_size: int, \n",
    "                 num_heads: int, \n",
    "                 num_layers: int, \n",
    "                 dropout: float = 0.1,\n",
    "                 max_seq_len: int = 5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input Embedding Pipeline\n",
    "        self.embedding_pipeline = nn.Sequential(\n",
    "            TokenEmbedding(vocab_size, embed_size),\n",
    "            PositionalEncoding(embed_size, dropout=dropout, maxlen=max_seq_len)\n",
    "        )\n",
    "\n",
    "        # Core Transformer Blocks\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_size, \n",
    "            nhead=num_heads, \n",
    "            dropout=dropout, \n",
    "            batch_first=False     # Expects (seq_len, batch_size, embed_size)\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Output Projection Layer\n",
    "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "        # Initialize weights after all layers are defined\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initializes model weights using Xavier uniform distribution.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_masks(src: Tensor, device):\n",
    "        \"\"\"\n",
    "        Creates the causal (look-ahead) and padding masks for the source sequence.\n",
    "        \"\"\"\n",
    "        seq_len = src.shape[0]\n",
    "        \n",
    "        # Causal mask: Prevents attending to future tokens.\n",
    "        # Shape: (seq_len, seq_len)\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len, device=device)\n",
    "        \n",
    "        # Padding mask: Prevents attending to <pad> tokens.\n",
    "        # Shape: (batch_size, seq_len)\n",
    "        padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "        \n",
    "        return causal_mask, padding_mask\n",
    "    \n",
    "\n",
    "    def forward(self, src: Tensor):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            src (Tensor): Input tensor of token IDs.\n",
    "                          Shape: (seq_len, batch_size)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output logits over the vocabulary.\n",
    "                    Shape: (seq_len, batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # Create masks based on the input tensor.\n",
    "        src_mask, src_padding_mask = self.create_masks(src, device=src.device)\n",
    "        \n",
    "        # Prepare input: Apply token embedding and positional encoding.\n",
    "        # src shape: (seq_len, batch_size) -> (seq_len, batch_size, embed_size)\n",
    "        src_emb = self.embedding_pipeline(src)\n",
    "\n",
    "        # Pass through the main Transformer blocks.\n",
    "        # Shape remains: (seq_len, batch_size, embed_size)\n",
    "        output = self.transformer_encoder(\n",
    "            src_emb, \n",
    "            mask=src_mask, \n",
    "            src_key_padding_mask=src_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Project to vocabulary space to get final logits.\n",
    "        # output shape: (seq_len, batch_size, embed_size) -> (seq_len, batch_size, vocab_size)\n",
    "        logits = self.lm_head(output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "a[-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_prompt(\n",
    "    prompt: str, \n",
    "    tokenizer,\n",
    "    vocab,\n",
    "    block_size,\n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    Encodes a string prompt into a tensor suitable for model input.\n",
    "\n",
    "    This function handles prompt validation, tokenization, truncation of long \n",
    "    prompts, and conversion to a correctly shaped tensor on the specified device.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The encoded prompt as a tensor of shape (sequence_length, 1).\n",
    "    \"\"\"\n",
    "    if not prompt or not prompt.strip():\n",
    "        raise ValueError(\"Prompt cannot be empty or contain only whitespace.\")\n",
    "\n",
    "    tokens = tokenizer(prompt)\n",
    "\n",
    "    # Truncate from the left if prompt exceeds the block size\n",
    "    if len(tokens) > block_size:\n",
    "        tokens = tokens[-block_size:]\n",
    "\n",
    "    # Convert tokens to numerical indices\n",
    "    indices = vocab(tokens)\n",
    "    # Shape: [seq_len] -> [seq_len, 1]\n",
    "    return torch.tensor(indices, dtype=torch.long, device=device).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(token_ids, vocab):\n",
    "    id_list = token_ids.flatten().tolist()\n",
    "    tokens = vocab.get_itos()(id_list)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(\n",
    "    model,\n",
    "    prompt,\n",
    "    tokenizer,\n",
    "    vocab,\n",
    "    block_size,\n",
    "    max_new_tokens,\n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a sequence of text autoregressively using a trained model.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text, including the prompt.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Encode the initial prompt\n",
    "    context = encode_prompt(\n",
    "        prompt=prompt,\n",
    "        tokenizer=tokenizer,\n",
    "        vocab=vocab,\n",
    "        block_size=block_size,\n",
    "        device=device\n",
    "    ) # Shape: (prompt_len, 1)\n",
    "\n",
    "    # The autoregressive generation loop\n",
    "    for _ in range(max_new_tokens):\n",
    "        context_cond = context[-block_size:]\n",
    "        \n",
    "        # Forward pass with the conditioned context\n",
    "        # logits shape: (current_seq_len, 1, vocab_size)\n",
    "        logits = model(context_cond)\n",
    "\n",
    "        # Get logits for the very last token in the sequence\n",
    "        # Shape: (1, vocab_size)\n",
    "        last_token_logits = logits[-1, :, :]\n",
    "        \n",
    "        # Greedily select the most likely next token\n",
    "        # Shape: (1, 1)\n",
    "        next_token = torch.argmax(last_token_logits, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Check for end-of-sequence token\n",
    "        if next_token.item() == EOS_IDX:\n",
    "            break\n",
    "            \n",
    "        # Append the predicted token to the running sequence\n",
    "        context = torch.cat([context, next_token], dim=0)\n",
    "\n",
    "    # Decode the final sequence of token IDs back to a string\n",
    "    generated_text = decode_tokens(context, vocab)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torchtext.data.utils._basic_english_normalize(line)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_SIZE = 256\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2\n",
    "BLOCK_SIZE = 10\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Instantiate the Model\n",
    "model = CustomGPTModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_size=EMBED_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    max_seq_len=BLOCK_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Prompt --- \n",
      "The sun rises in the\n",
      "\n",
      "--- Generated Text --- \n",
      "pseudo-shocking pseudo-shocking pseudo-shocking pseudo-shocking pseudo-shocking pseudo-shocking pseudo-shocking pseudo-shocking pseudo-shocking pseudo-shocking pseudo-shocking\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The sun rises in the\"\n",
    "print(f\"--- Prompt --- \\n{prompt}\\n\")\n",
    "\n",
    "generated_text = generate_text(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    tokenizer=tokenizer,\n",
    "    vocab=vocab,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    max_new_tokens=100,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(f\"--- Generated Text --- \\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the GPT-style language model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training Epoch\")\n",
    "\n",
    "    for src, tgt in progress_bar:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(src)\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        # logits: [seq_len, batch_size, vocab_size] -> [seq_len * batch_size, vocab_size]\n",
    "        # tgt:    [seq_len, batch_size] -> [seq_len * batch_size]\n",
    "        logits_flat = logits.reshape(-1, logits.shape[-1])\n",
    "        loss = criterion(logits_flat, tgt.reshape(-1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Evaluates the GPT-style language model.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[float, float, float]: Average loss, accuracy, and perplexity.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluating\")\n",
    "        for src, tgt in progress_bar:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            logits = model(src)\n",
    "            \n",
    "            # --- Loss Calculation ---\n",
    "            logits_flat = logits.reshape(-1, logits.shape[-1])\n",
    "            tgt_flat = tgt.reshape(-1)\n",
    "            loss = criterion(logits_flat, tgt_flat)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # --- Accuracy Calculation (Per-Token) ---\n",
    "            preds = torch.argmax(logits_flat, dim=1)\n",
    "            non_pad_mask = (tgt_flat != PAD_IDX)\n",
    "            total_correct += (preds[non_pad_mask] == tgt_flat[non_pad_mask]).sum().item()\n",
    "            total_tokens += non_pad_mask.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_tokens if total_tokens > 0 else 0\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return avg_loss, accuracy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_SIZE = 256\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "MAX_SEQ_LEN = 512\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# A simple learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1.0, gamma=0.95)\n",
    "\n",
    "print(\"Initializing model...\")\n",
    "model = CustomGPTModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_size=EMBED_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    max_seq_len=MAX_SEQ_LEN\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:   1%|          | 6/625 [00:00<00:10, 59.80it/s, loss=11.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 625/625 [00:10<00:00, 60.59it/s, loss=11.5]\n",
      "Evaluating: 100%|██████████| 157/157 [00:01<00:00, 118.52it/s]\n",
      "Training Epoch:   1%|          | 7/625 [00:00<00:09, 61.96it/s, loss=11.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "| End of Epoch   1 | Time: 11.64s | Train Loss: 11.519 | Val Loss: 11.517 | Val PPL: 100443.22 | Val Acc:  0.00%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 625/625 [00:10<00:00, 60.60it/s, loss=11.5]\n",
      "Evaluating: 100%|██████████| 157/157 [00:01<00:00, 122.16it/s]\n",
      "Training Epoch:   1%|          | 7/625 [00:00<00:10, 61.07it/s, loss=11.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "| End of Epoch   2 | Time: 11.60s | Train Loss: 11.519 | Val Loss: 11.518 | Val PPL: 100499.13 | Val Acc:  0.00%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 625/625 [00:10<00:00, 60.71it/s, loss=11.5]\n",
      "Evaluating: 100%|██████████| 157/157 [00:01<00:00, 122.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "| End of Epoch   3 | Time: 11.58s | Train Loss: 11.518 | Val Loss: 11.517 | Val PPL: 100440.23 | Val Acc:  0.00%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train_loss = train_one_epoch(model, train_dataloader, loss_fn, optimizer, scheduler, DEVICE)\n",
    "\n",
    "    val_loss, val_accuracy, val_perplexity = evaluate(model, val_dataloader, loss_fn, DEVICE)\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    \n",
    "    # --- 5. LOGGING RESULTS ---\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"| End of Epoch {epoch:3d} | Time: {epoch_duration:5.2f}s | \"\n",
    "          f\"Train Loss: {train_loss:5.3f} | Val Loss: {val_loss:5.3f} | \"\n",
    "          f\"Val PPL: {val_perplexity:8.2f} | Val Acc: {val_accuracy*100:5.2f}%\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Generation ---\n",
      "the meaning of life is spitied additive additive tarantinism tarantinism tarantinism tarantinism montford montford montford montford 15minutes 15minutes 15minutes 15minutes 15minutes 15minutes 15minutes 15minutes 15minutes struggles spanky montford confused confused confused --until --until rainforests rainforests rainforests rainforests rainforests tarantinism prepare prepare prepare prepare prepare prepare prepare prepare prepare prepare prepare rainforests rainforests rainforests rainforests rainforests rainforests rainforests rainforests semi-rural semi-rural semi-rural semi-rural rainforests rainforests rainforests rainforests rainforests pelicule pelicule pelicule rainforests rainforests rainforests rainforests rainforests sushant sushant tarantinism tarantinism t4 t4 t4 blecher blecher rainforests rainforests rainforests rainforests rainforests rainforests rainforests reviews reviews rainforests rainforests rainforests rainforests prosecutor boy-o-boy wisdom/knowledge wisdom/knowledge wisdom/knowledge reviews reviews reviews\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The meaning of life is\"\n",
    "generated_text = generate_text(model, prompt, tokenizer, vocab, MAX_SEQ_LEN, 100, DEVICE)\n",
    "print(\"\\n--- Example Generation ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
